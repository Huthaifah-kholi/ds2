{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor flow with scikit learn using tf.learn (classfier and regressor)  \n",
    "Note: changes to tf.learn API cause warnings and SKCompat is not fully functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "#tf.__version__   # '1.1.0'\n",
    "\n",
    "# we use this library to deal with files and folders\n",
    "import shutil\n",
    "\n",
    "# Cleans the given folder if it exists (model remembers past training and will not work if layers change)\n",
    "def clean_folder(folder):\n",
    "    try:\n",
    "        shutil.rmtree(folder)\n",
    "    except OSError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, log_loss, classification_report\n",
    "from sklearn.metrics import confusion_matrix , mean_squared_error, r2_score\n",
    "from sklearn.datasets import load_iris, load_boston\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target \n",
    "X = X.astype('float32')\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, train_size = 0.7, random_state=707)\n",
    "\n",
    "#val_monitor = tf.contrib.learn.monitors.ValidationMonitor(X_test, y_test, early_stopping_rounds=200)\n",
    "# Define the inputs for new api\n",
    "def get_train_inputs():\n",
    "    x = tf.constant(X_train)\n",
    "    y = tf.constant(y_train)\n",
    "    return x, y\n",
    "# Define the test inputs\n",
    "def get_test_inputs():\n",
    "    x = tf.constant(X_test)\n",
    "    y = tf.constant(y_test)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_keep_checkpoint_every_n_hours': 10000, '_environment': 'local', '_task_type': None, '_master': '', '_num_ps_replicas': 0, '_save_summary_steps': 100, '_num_worker_replicas': 0, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1\n",
      "}\n",
      ", '_task_id': 0, '_evaluation_master': '', '_keep_checkpoint_max': 5, '_model_dir': None, '_save_checkpoints_secs': 600, '_tf_random_seed': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000022D81906EF0>, '_save_checkpoints_steps': None, '_is_chief': True}\n"
     ]
    }
   ],
   "source": [
    "feature_columns = tf.contrib.learn.infer_real_valued_columns_from_input(X_train)\n",
    "# can also pecify that all features have real-value data\n",
    "#feature_columns = [tf.contrib.layers.real_valued_column(\"\", dimension=4)]\n",
    "\n",
    "iris_dir = '/tmp/iris_model'\n",
    "clean_folder(iris_dir)\n",
    "dnn_clf = tf.contrib.learn.DNNClassifier(hidden_units=[4,4,4], n_classes= 3,\n",
    "                                         feature_columns=feature_columns,\n",
    "                                            model_dir=iris_dir)\n",
    "#dnn_clf = tf.contrib.learn.SKCompat(dnn_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\software\\anaconda4\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\head.py:615: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /tmp/iris_model\\model.ckpt.\n",
      "INFO:tensorflow:step = 1, loss = 1.15226\n",
      "INFO:tensorflow:global_step/sec: 925.772\n",
      "INFO:tensorflow:step = 101, loss = 0.588292 (0.111 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 200 into /tmp/iris_model\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.319013.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DNNClassifier(params={'activation_fn': <function relu at 0x0000022DEB02A048>, 'dropout': None, 'hidden_units': [4, 4, 4], 'gradient_clip_norm': None, 'input_layer_min_slice_size': None, 'head': <tensorflow.contrib.learn.python.learn.estimators.head._MultiClassHead object at 0x0000022D81DF1240>, 'optimizer': None, 'feature_columns': (_RealValuedColumn(column_name='', dimension=4, default_value=None, dtype=tf.float32, normalizer=None),), 'embedding_lr_multipliers': None})"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dnn_clf.fit(input_fn=get_train_inputs, steps=200, monitors=[val_monitor])\n",
    "dnn_clf.fit(input_fn=get_train_inputs, steps=200) \n",
    "# batch_size=128, steps=None, max_steps=None, monitors=None\n",
    "#dnn_clf.fit(X_train, y_train, steps=None, max_steps= 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\software\\anaconda4\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\head.py:615: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "INFO:tensorflow:Starting evaluation at 2017-05-04-15:01:25\n",
      "INFO:tensorflow:Restoring parameters from /tmp/iris_model\\model.ckpt-200\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2017-05-04-15:01:25\n",
      "INFO:tensorflow:Saving dict for global step 200: accuracy = 1.0, global_step = 200, loss = 0.268876\n",
      "WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\n",
      "\n",
      "Test Accuracy: 1.000000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate accuracy.\n",
    "accuracy_score = dnn_clf.evaluate(input_fn=get_test_inputs,\n",
    "                                     steps=1)[\"accuracy\"]\n",
    "print(\"\\nTest Accuracy: {0:f}\\n\".format(accuracy_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/iris_model\\model.ckpt-200\n"
     ]
    }
   ],
   "source": [
    "y_pred = list(dnn_clf.predict_classes(input_fn = get_test_inputs))\n",
    "#accuracy = accuracy_score(y_test, y_pred)\n",
    "#accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/iris_model\\model.ckpt-200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.26887595396902825"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_proba = list(dnn_clf.predict_proba(input_fn = get_test_inputs))\n",
    "log_loss(y_test, y_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      " [[15  0  0]\n",
      " [ 0 15  0]\n",
      " [ 0  0 15]]\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        15\n",
      "          1       1.00      1.00      1.00        15\n",
      "          2       1.00      1.00      1.00        15\n",
      "\n",
      "avg / total       1.00      1.00      1.00        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_test,y_pred))\n",
    "print(\"Classification report:\\n\", classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Regressor using Boston dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "boston = load_boston()\n",
    "Xb, yb = shuffle(boston.data, boston.target, random_state=13)\n",
    "Xb_train, Xb_test, yb_train, yb_test = train_test_split(Xb, yb, train_size=0.75, random_state=717)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(Xb_train)\n",
    "Xb_train = scaler.transform(Xb_train)\n",
    "Xb_test = scaler.transform(Xb_test)\n",
    "def get_train_inputs_b():\n",
    "    x = tf.constant(Xb_train)\n",
    "    y = tf.constant(yb_train)\n",
    "    return x, y\n",
    "# Define the test inputs\n",
    "def get_test_inputs_b():\n",
    "    x = tf.constant(Xb_test)\n",
    "    y = tf.constant(yb_test)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_keep_checkpoint_every_n_hours': 10000, '_environment': 'local', '_task_type': None, '_master': '', '_num_ps_replicas': 0, '_save_summary_steps': 100, '_num_worker_replicas': 0, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1\n",
      "}\n",
      ", '_task_id': 0, '_evaluation_master': '', '_keep_checkpoint_max': 5, '_model_dir': None, '_save_checkpoints_secs': 600, '_tf_random_seed': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000022D85AD2C50>, '_save_checkpoints_steps': None, '_is_chief': True}\n"
     ]
    }
   ],
   "source": [
    "feature_columns = tf.contrib.learn.infer_real_valued_columns_from_input(Xb_train)\n",
    "\n",
    "boston_dir = '/tmp/boston_model'\n",
    "clean_folder(boston_dir)\n",
    "\n",
    "dnn_reg = tf.contrib.learn.DNNRegressor(\n",
    "      feature_columns=feature_columns, hidden_units=[20, 10, 10], model_dir = boston_dir, \n",
    "optimizer=tf.train.ProximalAdagradOptimizer(\n",
    "      learning_rate=0.1,\n",
    "      l1_regularization_strength=0.001),\n",
    "    activation_fn = tf.nn.relu6\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\software\\anaconda4\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\head.py:615: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /tmp/boston_model\\model.ckpt.\n",
      "INFO:tensorflow:step = 1, loss = 582.742\n",
      "INFO:tensorflow:global_step/sec: 721.905\n",
      "INFO:tensorflow:step = 101, loss = 11.0498 (0.140 sec)\n",
      "INFO:tensorflow:global_step/sec: 775.059\n",
      "INFO:tensorflow:step = 201, loss = 5.72366 (0.129 sec)\n",
      "INFO:tensorflow:global_step/sec: 858.068\n",
      "INFO:tensorflow:step = 301, loss = 4.54546 (0.117 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 400 into /tmp/boston_model\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 3.69198.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DNNRegressor(params={'activation_fn': <function relu6 at 0x0000022DEB323A60>, 'dropout': None, 'hidden_units': [20, 10, 10], 'gradient_clip_norm': None, 'input_layer_min_slice_size': None, 'head': <tensorflow.contrib.learn.python.learn.estimators.head._RegressionHead object at 0x0000022D85AD2668>, 'optimizer': <tensorflow.python.training.proximal_adagrad.ProximalAdagradOptimizer object at 0x0000022D890EF0F0>, 'feature_columns': (_RealValuedColumn(column_name='', dimension=13, default_value=None, dtype=tf.float64, normalizer=None),), 'embedding_lr_multipliers': None})"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit model\n",
    "dnn_reg.fit(input_fn = get_train_inputs_b, steps=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/boston_model\\model.ckpt-400\n",
      "MSE: 7.208418\n",
      "R^2: 0.913897\n"
     ]
    }
   ],
   "source": [
    "# Predict and score\n",
    "yb_predicted = list(dnn_reg.predict_scores(input_fn = get_test_inputs_b, as_iterable=True))\n",
    "score = mean_squared_error(yb_predicted, yb_test)\n",
    "Rsq = r2_score(yb_test, yb_predicted)\n",
    "print('MSE: {0:f}'.format(score))\n",
    "print('R^2: {0:f}'.format(Rsq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
