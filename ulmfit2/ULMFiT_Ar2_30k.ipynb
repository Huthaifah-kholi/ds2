{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ULMFiT_Ar2_30k.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "xC6OpYPPYvOA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This notebook creates an **Arabic ULMFiT** language model using **Fastai V1.x** and scripts in the same local directory (copied or adapted from [n-waves](https://github.com/n-waves/ulmfit-multilingual/blob/refactor/ulmfit/)).   \n",
        "It is a draft notebook that was working on Dec. 7, 2018 and no guarantee of maintenance or applicability to own cases.  \n",
        "The code was run on Google free colab instances. Some code cells have minor comments. \n",
        "If you have questions or need support, please use [fastai forum](https://forums.fast.ai/t/multilingual-ulmfit/28117/37). Please do not open issues - this is not a repository!"
      ]
    },
    {
      "metadata": {
        "id": "ZfzXhNTMjC_b",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# install latest torch and fastai for google colab or see https://github.com/fastai/fastai#installation\n",
        "\n",
        "!curl https://course-v3.fast.ai/setup/colab | bash \n",
        "\n",
        "# tokenizer Moses (also needed for xnli)\n",
        "# https://github.com/alvations/sacremoses\n",
        "!pip install sacremoses\n",
        "\n",
        "# cupy needs to be installed for QRNN\n",
        "!pip install cupy-cuda92\n",
        "\n",
        "# fire to run modules as command line interface\n",
        "!pip install fire       "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kvXCfzeYvE7R",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#---------- if fastai version is broken, remove and revert to working copy\n",
        "#!pip uninstall fastai \n",
        "#!pip install fastai==1.0.32\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lX3uP4KUd3kL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from fastai import *\n",
        "from fastai.text import * \n",
        "\n",
        "# only needed for diagnostics\n",
        "import fastai \n",
        "fastai.show_install(1) \n",
        "\n",
        "\n",
        "# for reproducibility (from fastai forum)\n",
        "# see https://forums.fast.ai/t/solved-reproducibility-where-is-the-randomness-coming-in/31628/5\n",
        "def random_seed(seed_value, use_cuda):\n",
        "    np.random.seed(seed_value) # cpu vars\n",
        "    torch.manual_seed(seed_value) # cpu  vars\n",
        "    random.seed(seed_value) # Python\n",
        "    if use_cuda: \n",
        "        torch.cuda.manual_seed(seed_value)\n",
        "        torch.cuda.manual_seed_all(seed_value) # gpu vars\n",
        "        torch.backends.cudnn.deterministic = True  #needed\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "        \n",
        "random_seed(42, True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ydD3u-_-Gy2l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# this shell calls the create_wikitext.py module, below\n",
        "!wget https://raw.githubusercontent.com/abedkhooli/ds2/master/ulmfit2/prepare_wiki.sh\n",
        "!wget https://raw.githubusercontent.com/abedkhooli/ds2/master/ulmfit2/create_wikitext.py   \n",
        "\n",
        "# these scripts are also used (some renamaed from original to work in same folder)\n",
        "!wget https://raw.githubusercontent.com/abedkhooli/ds2/master/ulmfit2/fastai_contrib_utils.py\n",
        "!wget https://raw.githubusercontent.com/abedkhooli/ds2/master/ulmfit2/postprocess_wikitext.py \n",
        "!wget https://raw.githubusercontent.com/abedkhooli/ds2/master/ulmfit2/fastai_contrib_data.py\n",
        "!wget https://raw.githubusercontent.com/abedkhooli/ds2/master/ulmfit2/fastai_contrib_learner.py\n",
        "!wget https://raw.githubusercontent.com/abedkhooli/ds2/master/ulmfit2/fastai_contrib_models.py\n",
        "!wget https://raw.githubusercontent.com/abedkhooli/ds2/master/ulmfit2/pretrain_lm.py\n",
        "    \n",
        "# for classifier training    \n",
        "!wget https://raw.githubusercontent.com/abedkhooli/ds2/master/ulmfit2/train_clas.py  \n",
        "# xnli dataset\n",
        "!wget https://raw.githubusercontent.com/abedkhooli/ds2/master/ulmfit2/prepare_xnli.sh    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Wu2pt-EPhaDl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# run shell silently (also supressed logger in wiki extractor)\n",
        "# creates data/wiki_dumps (.bz2) \n",
        "# for Arabic:\n",
        "# and data/wiki_extr/ar (AA - AL, .. folders with 100 wiki_xx in each)\n",
        "# and data/wiki/ar-100,data/wiki/ar-2,data/wiki/ar-all \n",
        "# warnings from WikiExtractor.py (cloned in shell). See:\n",
        "# https://github.com/attardi/wikiextractor/blob/master/WikiExtractor.py#L666\n",
        "\n",
        "# lots of info to stdout, turned off with > /dev/null\n",
        "!bash prepare_wiki.sh > /dev/null"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YOGqqLK6Igjj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# connect g drive to store files, session die or disconnect. Wiki extraction takes time\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gzg_4bxqRF53",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#------------ token files to drive\n",
        "!cp -a data/wiki/. '/gdrive/My Drive/ulmfit/ar2/'\n",
        "#------------ raw files to drive\n",
        "!cp -a data/wiki_extr/. '/gdrive/My Drive/ulmfit/ar2/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yRjco1C_xq08",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# ----------- create ar-unk files from small corpus ---------\n",
        "!python -m postprocess_wikitext \"data/wiki/ar-2\" 'ar'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5O8rwBmAgFeM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# save the unk tokens (source and target may differ - check you env) \n",
        "!cp data/wiki-unk/ar.wiki.test.tokens '/gdrive/My Drive/ulmfit/ar2/unk/'\n",
        "!cp data/wiki-unk/ar.wiki.train.tokens '/gdrive/My Drive/ulmfit/ar2/unk/'\n",
        "!cp data/wiki-unk/ar.wiki.valid.tokens '/gdrive/My Drive/ulmfit/ar2/unk/'\n",
        "\n",
        "# assume you want to work from drive after disconnect\n",
        "!rm -rf data/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QAWTylJNoXwV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# get unk wiki files from g drive\n",
        "!cp -a '/gdrive/My Drive/ulmfit/ar2/unk/.' data/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Zikc0uVncnyJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# here, I was training for 30k vocab. If need to start over and clear folders\n",
        "!rm -rf tmp\n",
        "!rm -rf data/models/v30k\n",
        "!rm -rf models/*"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cY1A65yV-g16",
        "colab_type": "code",
        "outputId": "7f8a5541-de01-4182-e727-eada15e4ec61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "cell_type": "code",
      "source": [
        "#----- qrnn (quasi rnn, faster) and bidir (sentencepiece) not working at this time\n",
        "#----- qrnn trains but model not read by fastai, bidir causes batch matching error\n",
        "\n",
        "import pretrain_lm\n",
        "import train_clas\n",
        "\n",
        "exp = pretrain_lm.LMHyperParams(dataset_path='data', \n",
        "                                base_lm_path=None, bidir=False, \n",
        "                                qrnn=False, tokenizer='v', max_vocab=30000, \n",
        "                                emb_sz=400, nh=1150, nl=3, clip=0.2, \n",
        "                                bptt=70, bs=64, lang='ar', name='Arabic')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Batch size: 64\n",
            "Max vocab: 30000\n",
            "Cache dir: data/models/v30k\n",
            "Model dir: data/models/v30k/lstm_Arabic.m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0xgVDnajGjKr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#===============\n",
        "#---- Runtime/restart runtime may help if out of memory\n",
        "#---- qrnn causes memory issue sometimes, keep off for now\n",
        "# learning rate (in code, not a param) is probably too small.\n",
        "#===============\n",
        "\n",
        "# use this to train model\n",
        "#exp.train_lm(num_epochs=10, drop_mult=0.3) \n",
        "\n",
        "# if you want to report logloss and perplexity. Method brought back into class, hard wired for moses no sp.\n",
        "exp.validate_lm(num_epochs=10, drop_mult=0.3) # this calls train_lm\n",
        "\n",
        "# for Arabic small corpus:\n",
        "# Test logloss: 3.2080156803131104 perplexity: 24.729965209960938\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CuwlSNpoHbVG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}